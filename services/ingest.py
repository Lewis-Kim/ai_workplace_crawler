import os
import logging
from datetime import datetime
from sqlalchemy.orm import Session

from models.meta import MetaTable
from models.content import ContentTable
from models.ImageTable import ImageTable

from services.loaders.pdf_loader import PDFLoader
from services.loaders.txt_loader import TXTLoader
from services.loaders.excel_loader import ExcelLoader
from services.loaders.csv_loader import CSVLoader
from services.loaders.docx_loader import DOCXLoader
from services.loaders.image_ocr_loader import ImageOCRLoader

from services.chunking import chunk_text
from services.utils.file_hash import file_sha1
from services.images.image_extractor import extract_images

from qdrant_client import QdrantClient
from vector.collection_manager import ensure_collection
from vector.realtime_vector import insert_vector


# =================================================
# ğŸ”§ Qdrant / Embedding ì„¤ì • (ğŸ”¥ í•µì‹¬)
# =================================================
QDRANT_HOST = "192.168.50.32"
QDRANT_PORT = 6333

BASE_COLLECTION = "documents"
MODEL_KEY = "nomic"   # â­ ëª¨ë¸ ë³€ê²½ì€ ì—¬ê¸°ë§Œ

qdrant_client = QdrantClient(
    host=QDRANT_HOST,
    port=QDRANT_PORT,
    timeout=30
)

# ğŸš€ ì•± ì‹œì‘ ì‹œ 1íšŒë§Œ ì‹¤í–‰
COLLECTION_NAME = ensure_collection(
    client=qdrant_client,
    base_collection=BASE_COLLECTION,
    model_key=MODEL_KEY
)


# =================================================
# logging ì„¤ì •
# =================================================
logger = logging.getLogger("ingest")
logger.setLevel(logging.INFO)

if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter(
        "[%(asctime)s] [%(levelname)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)


# =================================================
# loader map
# =================================================
LOADER_MAP = {
    "pdf": PDFLoader(),
    "txt": TXTLoader(),
    "xlsx": ExcelLoader(),
    "xls": ExcelLoader(),
    "csv": CSVLoader(),
    "docx": DOCXLoader(),
    "jpg": ImageOCRLoader(),
    "jpeg": ImageOCRLoader(),
    "png": ImageOCRLoader(),
}


# =================================================
# ingest main
# =================================================
def ingest_file(file_path: str, source: str, db: Session):

    logger.info(f"[START] ingest_file | file={file_path}")

    # -------------------------------------------------
    # 1ï¸âƒ£ íŒŒì¼ íƒ€ì… í™•ì¸
    # -------------------------------------------------
    ext = os.path.splitext(file_path)[1].lower().lstrip(".")

    if ext not in LOADER_MAP:
        logger.error(f"[STOP] unsupported file type: {ext}")
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ íƒ€ì…: {ext}")

    logger.info(f"[STEP 1] file type detected: {ext}")

    # -------------------------------------------------
    # 2ï¸âƒ£ íŒŒì¼ í•´ì‹œ ê³„ì‚°
    # -------------------------------------------------
    file_hash = file_sha1(file_path)
    logger.info(f"[STEP 2] file hash calculated: {file_hash}")

    # -------------------------------------------------
    # 3ï¸âƒ£ ì¤‘ë³µ íŒŒì¼ ì²´í¬
    # -------------------------------------------------
    exists = db.query(MetaTable).filter(
        MetaTable.file_hash == file_hash
    ).first()

    if exists:
        logger.warning(
            f"[SKIP] duplicate file | file={file_path}, doc_id={exists.seq_id}"
        )
        return exists.seq_id

    # -------------------------------------------------
    # 4ï¸âƒ£ meta_table INSERT
    # -------------------------------------------------
    meta = MetaTable(
        title=os.path.basename(file_path),
        file_type=ext,
        sorce=source,
        create_dt=datetime.now(),
        file_hash=file_hash
    )

    db.add(meta)
    db.commit()
    db.refresh(meta)

    logger.info(f"[STEP 3] meta inserted | doc_id={meta.seq_id}")

    # -------------------------------------------------
    # 5ï¸âƒ£ ì´ë¯¸ì§€ ì¶”ì¶œ + images INSERT
    # -------------------------------------------------
    image_root = "images"    
    image_dir = f"{image_root}/{meta.seq_id}"

    logger.info(f"[STEP 4] image extraction start")

    images = extract_images(
        file_path=file_path,
        output_dir=image_dir   # âœ… ë°˜ë“œì‹œ doc_id ê²½ë¡œ
    )

    for idx, img in enumerate(images, start=1):
        image_name = img["image"]
        image_ext = os.path.splitext(image_name)[1].lstrip(".")

        db.add(ImageTable(
            doc_id=meta.seq_id,
            page_no=img.get("page"),
            image_no=idx,
            image_path=f"{image_dir}/{image_name}",
            image_name=image_name,
            image_ext=image_ext
        ))

    db.commit()

    logger.info(
        f"[STEP 4 DONE] images inserted | count={len(images)}"
    )

    # -------------------------------------------------
    # 6ï¸âƒ£ í…ìŠ¤íŠ¸ ë¡œë“œ â†’ chunk â†’ DB + ğŸ”¥ Vector Insert
    # -------------------------------------------------
    loader = LOADER_MAP[ext]

    unit_count = 0
    chunk_count = 0

    logger.info("[STEP 5] text loading & chunking start")

    for unit_no, text in loader.load(file_path):
        unit_count += 1

        chunks = chunk_text(text)

        for idx, chunk in enumerate(chunks, start=1):
            chunk_count += 1

            content = ContentTable(
                doc_id=meta.seq_id,
                page_no=unit_no,
                chunk_no=idx,
                content=chunk
            )
            db.add(content)
            db.commit()
            db.refresh(content)

            # ğŸ”¥ Qdrant Vector Insert (ì»¬ë ‰ì…˜/ëª¨ë¸ ëª…ì‹œ)
            try:
                insert_vector(
                    collection_name=COLLECTION_NAME,
                    model_key=MODEL_KEY,
                    content_id=content.content_id,
                    doc_id=meta.seq_id,
                    page_no=unit_no,
                    chunk_no=idx,
                    text=chunk[:1500],  # ğŸ”’ ì•ˆì „ ê¸¸ì´ ì œí•œ
                )
            except Exception as ve:
                logger.error(
                    f"[VECTOR FAIL] content_id={content.content_id} | {ve}"
                )

    logger.info(
        f"[STEP 5 DONE] text stored | units={unit_count}, chunks={chunk_count}"
    )

    # -------------------------------------------------
    # 7ï¸âƒ£ ì™„ë£Œ
    # -------------------------------------------------
    logger.info(
        f"[END] ingest completed | doc_id={meta.seq_id}, "
        f"images={len(images)}, chunks={chunk_count}"
    )

    return meta.seq_id
